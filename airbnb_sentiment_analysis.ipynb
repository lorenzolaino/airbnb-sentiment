{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AirBnB Sentiment Analysis - Dataset generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Project Scope\n",
    " - Use of sentiment analysis, of the reviews of each ad, to view the evaluation of the ad\n",
    "    itself.\n",
    "\n",
    " - Search for relationships between the price of a room and the day of the week, holidays,\n",
    "    and time of year, and relationships between the price and the characteristics of a\n",
    "    room to make a forecast.\n",
    "\n",
    "Dataset: https://www.kaggle.com/brittabettendorf/berlin-airbnb-data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile36 as zipfile\n",
    "import langdetect\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Import of the reviews' dataset\n",
    "\n",
    "The first step concerns the download of the datasets.\n",
    "In particular, for this purpose, the Kaggle APIs are used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!kaggle datasets download -d brittabettendorf/berlin-airbnb-data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile('berlin-airbnb-data.zip')\n",
    "dfReviews = pd.read_csv(zf.open('reviews_summary.csv'))\n",
    "dfReviews.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Null data-points removal\n",
    "\n",
    "Once the dataset is available, it is needed to check whether there are some null data-points."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfReviews.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfNullReviews = dfReviews[dfReviews['comments'].isnull()]\n",
    "print(f'Number of null comments: {dfNullReviews.shape[0]}')\n",
    "dfNullReviews.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfReviews.dropna(axis=0, how='any', inplace=True)\n",
    "dfReviews.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Lowercase conversion\n",
    "\n",
    "After the null data-points removal operation, it is needed to convert all the comments\n",
    "into lowercase strings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfReviews['comments'] = dfReviews.apply(lambda x: x['comments'].lower(), axis=1)\n",
    "dfReviews.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Reviews' language detection\n",
    "\n",
    "Since the comments are written in many languages, it can be useful to detect the language\n",
    "of each comment.\n",
    "This operation allows the selection of the comments based on their language (and also an\n",
    "eventual translation of all the comments into a common language).\n",
    "\n",
    "In order to detect the language of the comments, the langdetect library is used.\n",
    "\n",
    "The first step of this operation concerns the definition of a method that"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_lang_from_comment(dataframe):\n",
    "    list_langs = []\n",
    "    for index, comment in dataframe['comments'].iteritems():\n",
    "        if index % 5000 == 0:\n",
    "            print(f'Processed {index} rows...')\n",
    "        try:\n",
    "            comment_lang = langdetect.detect(comment[:50])\n",
    "            list_langs.append(comment_lang)\n",
    "        except:\n",
    "            list_langs.append(\"None\")\n",
    "\n",
    "    return list_langs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the language for each comment is detected, it is added as a new column to the already\n",
    "existing dataframe. Then the resulting dataframe is saved into a .csv file.\n",
    "\n",
    "Since this operation is very time-consuming, it is checked whether the operation has\n",
    "already been executed, and the results have been saved into a .csv file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.exists('reviews_summary_langs.csv'):\n",
    "    dfReviews = pd.read_csv('reviews_summary_langs.csv')\n",
    "else:\n",
    "    dfReviews['Lang'] = get_lang_from_comment(dfReviews)\n",
    "    dfReviews.to_csv('reviews_summary_langs.csv', sep=\",\", index=False, header=True)\n",
    "\n",
    "dfReviews.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfReviews['Lang'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The rows in which the 'Lang' column shows the value 'None' are the ones that in the previous\n",
    "step have thrown some problems.\n",
    "In particular, the possible problems are the inability of the used technique to detect\n",
    "their language or the too-narrow length of the review."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfNoneLangReviews = dfReviews[dfReviews['Lang'] == 'None']\n",
    "print(f'Number of reviews with None language: {dfNoneLangReviews.shape[0]}')\n",
    "print(f'Percentage of reviews with None language: '\n",
    "      f'{round(dfNoneLangReviews.shape[0] * 100 / dfReviews.shape[0],2)}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.1 English reviews selection\n",
    "\n",
    "The reviews written in english language are the interesting ones for this analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfEnglishReviews = dfReviews[dfReviews['Lang'] == 'en']\n",
    "dfEnglishReviews.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfEnglishReviews.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Duplicates removal\n",
    "\n",
    "The first step requires the removal of the duplicated reviews."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Number of English reviews: {}'.format(dfEnglishReviews.shape[0]))\n",
    "print('Number of unique English reviews: {}'.format(len(dfEnglishReviews['comments'].unique())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfEnglishReviews = dfEnglishReviews.drop_duplicates(subset='comments')\n",
    "print(f'Number of reviews after the duplicated removal: {dfEnglishReviews.shape[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 Non-English words removal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfEnglishReviews['comments'].iloc[172]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from re import sub\n",
    "\n",
    "dfEnglishReviews['comments'] = dfEnglishReviews.apply(\n",
    "    lambda x: sub(r\"[^A-Za-z]\", \" \", x['comments']), axis=1)\n",
    "dfEnglishReviews['comments'].iloc[172]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.6 Tokenization\n",
    "\n",
    "In order to prepare the data for the analysis model, it is needed to perform a tokenization\n",
    "operation.\n",
    "For this purpose, the 'gensim' library is used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizedEnglishReviews = dfEnglishReviews.apply(\n",
    "    lambda x: gensim.utils.simple_preprocess(str(x['comments'])), axis=1)\n",
    "tokenizedEnglishReviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.7 Normalization\n",
    "\n",
    "Another important step concerns the normalization of the reviews.\n",
    "For this purpose, the 'nltk' library is used.\n",
    "\n",
    "In particular, the 'wordnet' and 'average_perceptron_tagger' packages are downloaded from\n",
    "the 'nltk' resources.\n",
    "The first package provides a 'Lemmatizer' that, given a word, converts it into its base form.\n",
    "The second package provides a method that, given a word, returns a tag representing its\n",
    "grammatical type."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def lemmatize_reviews(tokenized_reviews):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_reviews = []\n",
    "    for tokens_review in tokenized_reviews:\n",
    "        lemmatized_review = []\n",
    "        for word, tag in pos_tag(tokens_review):\n",
    "            if tag.startswith('NN'):\n",
    "                pos = 'n'\n",
    "            elif tag.startswith('VB'):\n",
    "                pos = 'v'\n",
    "            else:\n",
    "                pos = 'a'\n",
    "            lemmatized_review.append(lemmatizer.lemmatize(word, pos))\n",
    "        lemmatized_reviews.append(lemmatized_review)\n",
    "\n",
    "    return lemmatized_reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lemmatizedTokenizedEnglishReviews = lemmatize_reviews(tokenizedEnglishReviews)\n",
    "lemmatizedTokenizedEnglishReviews[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Sentiment Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Bigrams generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "phrases = Phrases(lemmatizedTokenizedEnglishReviews, min_count=3, progress_per=50000)\n",
    "\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "bigramReviews = bigram[lemmatizedTokenizedEnglishReviews]\n",
    "\n",
    "bigramReviews[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dictWordFreq = defaultdict(int)\n",
    "for review in bigramReviews:\n",
    "    for i in review:\n",
    "        dictWordFreq[i] += 1\n",
    "\n",
    "len(dictWordFreq)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Eventually, show an example of item in word_freq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted(dictWordFreq, key=dictWordFreq.get, reverse=True)[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Word2Vec model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2vModel = Word2Vec(min_count=20,\n",
    "                    window=4,\n",
    "                    vector_size=300,\n",
    "                    sample=6e-5,\n",
    "                    alpha=0.03,\n",
    "                    min_alpha=0.0007,\n",
    "                    negative=20,\n",
    "                    workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2vModel.build_vocab(bigramReviews, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = time()\n",
    "\n",
    "w2vModel.train(bigramReviews,\n",
    "               total_examples=w2vModel.corpus_count,\n",
    "               epochs=30,\n",
    "               report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# w2vModel.save(\"word2vec.model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an example, it is possible to show the most similar words to a given word.\n",
    "This step allows to have a first look at the goodness of the Word2Vec model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2vModel.wv.most_similar(positive=[\"apartment\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Clustering model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "kmeansModel2Clusters = KMeans(n_clusters=2, max_iter=1000, random_state=42, n_init=50)\n",
    "kmeansModel2Clusters.fit(X=w2vModel.wv.vectors.astype('double'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2vModel.wv.similar_by_vector(kmeansModel2Clusters.cluster_centers_[0],\n",
    "                              topn=10,\n",
    "                              restrict_vocab=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "negativeClusterIndex = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfWords2Clusters = pd.DataFrame(\n",
    "    w2vModel.wv.key_to_index.keys())\n",
    "\n",
    "dfWords2Clusters.columns = ['words']\n",
    "\n",
    "dfWords2Clusters['vectors'] = \\\n",
    "    dfWords2Clusters['words'].apply(\n",
    "        lambda x: w2vModel.wv[f'{x}'])\n",
    "\n",
    "dfWords2Clusters['cluster'] = \\\n",
    "    dfWords2Clusters['vectors'].apply(\n",
    "        lambda x: kmeansModel2Clusters.predict([np.array(x)]))\n",
    "\n",
    "dfWords2Clusters['cluster'] = \\\n",
    "    dfWords2Clusters['cluster'].apply(\n",
    "        lambda x: x[0])\n",
    "\n",
    "dfWords2Clusters.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfWords2Clusters['cluster_value'] = [\n",
    "    -1 if i==negativeClusterIndex else 1\n",
    "    for i in dfWords2Clusters['cluster']]\n",
    "\n",
    "dfWords2Clusters['closeness_score'] = \\\n",
    "    dfWords2Clusters.apply(\n",
    "        lambda x: 1/(kmeansModel2Clusters.transform([x.vectors]).min()),\n",
    "        axis=1)\n",
    "\n",
    "dfWords2Clusters['sentiment_coeff'] = \\\n",
    "    dfWords2Clusters['closeness_score'] * \\\n",
    "    dfWords2Clusters['cluster_value']\n",
    "\n",
    "dfWords2Clusters[\n",
    "    dfWords2Clusters['cluster_value'] == -1].head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 TF-IDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfCleanedReviews = pd.DataFrame(\n",
    "    [' '.join(review) for review in lemmatizedTokenizedEnglishReviews],\n",
    "    columns=['comments'])\n",
    "\n",
    "dfCleanedReviews.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(norm=None)\n",
    "transformed = tfidf.fit_transform(\n",
    "    dfCleanedReviews['comments'].tolist())\n",
    "features = pd.Series(tfidf.get_feature_names())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_tfidf_dictionary(x, transformed_file, features_file):\n",
    "    \"\"\"\n",
    "    create dictionary for each input sentence x, where each word has assigned its tfidf score\n",
    "\n",
    "    inspired  by function from this wonderful article:\n",
    "    https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "\n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "    \"\"\"\n",
    "    vector_coo = transformed_file[x.name].tocoo()\n",
    "    vector_coo.col = features_file.iloc[vector_coo.col].values\n",
    "    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n",
    "    return dict_from_coo\n",
    "\n",
    "def replace_tfidf_words(x, transformed_file, features_file):\n",
    "    \"\"\"\n",
    "    replacing each word with it's calculated tfidf dictionary with scores of each word\n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "    \"\"\"\n",
    "    dictionary = create_tfidf_dictionary(x, transformed_file, features_file)\n",
    "    try:\n",
    "        res = list(map(lambda y:dictionary[f'{y}'], x['comments'].split()))\n",
    "    except KeyError:\n",
    "        res = [0 for i in x['comments'].split()]\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfidfScores = dfCleanedReviews.apply(\n",
    "    lambda x: replace_tfidf_words(x, transformed, features), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5 Closeness score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dictSentiment2Clusters = dict(zip(\n",
    "    dfWords2Clusters['words'].values,\n",
    "    dfWords2Clusters['sentiment_coeff'].values))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def replace_sentiment_words(word, sentiment_dict):\n",
    "    \"\"\"\n",
    "    replacing each word with its associated sentiment score from sentiment dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        out = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        out = 0\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "closenessScores2Clusters = \\\n",
    "    dfCleanedReviews['comments'].apply(\n",
    "        lambda x: list(map(\n",
    "            lambda y: replace_sentiment_words(y, dictSentiment2Clusters),\n",
    "            x.split())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.6 Sentiment score computation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfSentiment2ClustersTfidfReviews = \\\n",
    "    pd.DataFrame([closenessScores2Clusters,\n",
    "                  tfidfScores,\n",
    "                  dfCleanedReviews['comments']]).T\n",
    "\n",
    "dfSentiment2ClustersTfidfReviews.columns = \\\n",
    "    ['sentiment_coeff', 'tfidf_scores', 'review']\n",
    "\n",
    "dfSentiment2ClustersTfidfReviews['sentiment_rate'] = \\\n",
    "    dfSentiment2ClustersTfidfReviews.apply(\n",
    "        lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']),\n",
    "        axis=1)\n",
    "\n",
    "dfSentiment2ClustersTfidfReviews['prediction'] =\\\n",
    "    (dfSentiment2ClustersTfidfReviews['sentiment_rate'] > 0)\\\n",
    "        .astype('int8')\n",
    "\n",
    "dfSentiment2ClustersTfidfReviews.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is also possible to show as an example the top-5 negative reviews, according to our\n",
    "sentiment prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfNegativeSentiment = dfSentiment2ClustersTfidfReviews[\n",
    "    dfSentiment2ClustersTfidfReviews['prediction'] == 0].sort_values(\n",
    "        by=['sentiment_rate'])\n",
    "\n",
    "print('Top-5 negative reviews:')\n",
    "dfNegativeSentiment['review'].head().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, the dataset is saved into a .csv file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dfSentiment2ClustersTfidfReviews.to_csv(\n",
    "#     'sentiment_dataset_2_clusters.csv',\n",
    "#     sep=',', index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.7 TextBlob"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "textblobSentiment = dfCleanedReviews['comments'].apply(\n",
    "    lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "textblobSentiment.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfSentiment2ClustersTfidfReviews['textblob_sentiment'] = \\\n",
    "    textblobSentiment\n",
    "\n",
    "dfSentiment2ClustersTfidfReviews['textblob_prediction'] = \\\n",
    "    (dfSentiment2ClustersTfidfReviews['textblob_sentiment'] > 0)\\\n",
    "        .astype('int8')\n",
    "\n",
    "dfSentiment2ClustersTfidfReviews.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.8 Sentiment Analysis Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_test_scores(predictions, labels):\n",
    "\n",
    "    df_conf_matrix = pd.DataFrame(confusion_matrix(labels, predictions))\n",
    "\n",
    "    print(df_conf_matrix)\n",
    "\n",
    "    test_scores = accuracy_score(labels, predictions), \\\n",
    "                  precision_score(labels, predictions), \\\n",
    "                  recall_score(labels, predictions), \\\n",
    "                  f1_score(labels, predictions)\n",
    "\n",
    "    return test_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testScores2ClustersSentiment = compute_test_scores(\n",
    "    dfSentiment2ClustersTfidfReviews['prediction'],\n",
    "    dfSentiment2ClustersTfidfReviews['textblob_prediction'])\n",
    "\n",
    "dfTestScores2ClustersSentiment = pd.DataFrame([testScores2ClustersSentiment])\n",
    "dfTestScores2ClustersSentiment.columns = ['accuracy', 'precision', 'recall', 'f1']\n",
    "dfTestScores2ClustersSentiment = dfTestScores2ClustersSentiment.T\n",
    "dfTestScores2ClustersSentiment.columns = ['scores']\n",
    "\n",
    "print('Scores for sentiment analysis with 2 clusters and no stopwords: ')\n",
    "dfTestScores2ClustersSentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The sentiment analysis with 2 clusters shows bad results.\n",
    "So, we try to compute both the elbow and the silhouette methods in order to check\n",
    "whether the clustering of the words can be performed with better results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.9 Clustering evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def kmeans_elbow_method(vectors):\n",
    "    wcss = []\n",
    "    for i in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "        kmeans.fit(X=vectors)\n",
    "\n",
    "        # inertia_ is sum of squared distance of samples to its closest cluster centers.\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        print(\"inertia_\", kmeans.inertia_)\n",
    "\n",
    "    plt.plot(range(1, 11), wcss)\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kmeans_elbow_method(w2vModel.wv.vectors.astype('double'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def kmeans_silhouette(X,range_clusters):\n",
    "    for i, k in range_clusters :\n",
    "\n",
    "        # Run the Kmeans algorithm\n",
    "        km = KMeans(n_clusters = k, init = 'k-means++', random_state = 42)\n",
    "\n",
    "        km.fit(X)\n",
    "        labels = km.predict(X)\n",
    "\n",
    "        print(\"For n_clusters =\", k,\n",
    "                  \"The computed average silhouette_score is :\",\n",
    "              silhouette_score(X, labels, metric='euclidean'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rangeClusters = enumerate([2,3,4,5,6,7,8,9,10])\n",
    "# kmeans_silhouette(w2vModel.wv.vectors.astype('double'), rangeClusters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Sentiment analysis with 3 clusters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Clustering model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kmeansModel3Clusters = KMeans(n_clusters=3, max_iter=1000, random_state=42, n_init=50)\n",
    "kmeansModel3Clusters.fit(X=w2vModel.wv.vectors.astype('double'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2vModel.wv.similar_by_vector(\n",
    "    kmeansModel3Clusters.cluster_centers_[2], topn=10, restrict_vocab=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "negativeClusterIndex = 2\n",
    "positiveClusterIndex = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfWords3Clusters = pd.DataFrame(\n",
    "    w2vModel.wv.key_to_index.keys())\n",
    "\n",
    "dfWords3Clusters.columns = ['words']\n",
    "\n",
    "dfWords3Clusters['vectors'] = \\\n",
    "    dfWords3Clusters['words'].apply(\n",
    "        lambda x: w2vModel.wv[f'{x}'])\n",
    "\n",
    "dfWords3Clusters['cluster'] = \\\n",
    "    dfWords3Clusters['vectors'].apply(\n",
    "        lambda x: kmeansModel3Clusters.predict([np.array(x)]))\n",
    "\n",
    "dfWords3Clusters.cluster = \\\n",
    "    dfWords3Clusters['cluster'].apply(\n",
    "        lambda x: x[0])\n",
    "\n",
    "dfWords3Clusters.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfWords3Clusters['cluster_value'] = \\\n",
    "    [-1 if i==negativeClusterIndex\n",
    "     else 1 if i==positiveClusterIndex else 0\n",
    "     for i in dfWords3Clusters['cluster']]\n",
    "\n",
    "dfWords3Clusters['closeness_score'] = \\\n",
    "    dfWords3Clusters.apply(\n",
    "        lambda x: 1/(kmeansModel3Clusters.transform([x.vectors]).min()),\n",
    "        axis=1)\n",
    "\n",
    "dfWords3Clusters['sentiment_coeff'] = \\\n",
    "    dfWords3Clusters['closeness_score'] * \\\n",
    "    dfWords3Clusters['cluster_value']\n",
    "\n",
    "dfWords3Clusters[\n",
    "    dfWords3Clusters['cluster_value'] == -1].head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Closeness score\n",
    "\n",
    "The TF-IDF technique is not applied because the results would be the set of words\n",
    "composing the reviews has not changed.\n",
    "\n",
    "..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dictSentiment3Clusters = dict(zip(\n",
    "    dfWords3Clusters['words'].values,\n",
    "    dfWords3Clusters['sentiment_coeff'].values))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "closenessScores3Clusters = \\\n",
    "    dfCleanedReviews['comments'].apply(\n",
    "        lambda x: list(map(\n",
    "            lambda y: replace_sentiment_words(y, dictSentiment3Clusters),\n",
    "            x.split())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 Sentiment score computation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfSentiment3ClustersTfidfReviews = pd.DataFrame(\n",
    "    [closenessScores3Clusters,\n",
    "     tfidfScores,\n",
    "     dfCleanedReviews['comments']]).T\n",
    "\n",
    "dfSentiment3ClustersTfidfReviews.columns = \\\n",
    "    ['sentiment_coeff', 'tfidf_scores', 'review']\n",
    "\n",
    "dfSentiment3ClustersTfidfReviews['sentiment_rate'] = \\\n",
    "    dfSentiment3ClustersTfidfReviews.apply(\n",
    "        lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']),\n",
    "        axis=1)\n",
    "\n",
    "dfSentiment3ClustersTfidfReviews['prediction'] = \\\n",
    "    (dfSentiment3ClustersTfidfReviews['sentiment_rate']>0)\\\n",
    "        .astype('int8')\n",
    "\n",
    "dfSentiment3ClustersTfidfReviews.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dfSentiment3ClustersTfidfReviews.to_csv(\n",
    "#     'sentiment_dataset_3_clusters.csv',\n",
    "#     sep=',', index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfNegativeSentiment = dfSentiment3ClustersTfidfReviews[\n",
    "    dfSentiment3ClustersTfidfReviews['prediction'] == 0].sort_values(\n",
    "        by=['sentiment_rate'])\n",
    "\n",
    "dfNegativeSentiment['review'].head().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4 TextBlob\n",
    "\n",
    "Even in this case, the list of values concerning the TextBlob sentiment prediction has\n",
    "not changed.\n",
    "So, the same list is used."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfSentiment3ClustersTfidfReviews['textblob_sentiment'] = \\\n",
    "    textblobSentiment\n",
    "\n",
    "dfSentiment3ClustersTfidfReviews['textblob_prediction'] = \\\n",
    "    (dfSentiment3ClustersTfidfReviews['textblob_sentiment'] > 0)\\\n",
    "        .astype('int8')\n",
    "\n",
    "dfSentiment3ClustersTfidfReviews.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.5 Sentiment evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testScores3ClustersSentiment = compute_test_scores(\n",
    "    dfSentiment3ClustersTfidfReviews['prediction'],\n",
    "    dfSentiment3ClustersTfidfReviews['textblob_prediction'])\n",
    "\n",
    "dfTestScores3ClustersSentiment = pd.DataFrame([testScores3ClustersSentiment])\n",
    "dfTestScores3ClustersSentiment.columns = ['accuracy', 'precision', 'recall', 'f1']\n",
    "dfTestScores3ClustersSentiment = dfTestScores3ClustersSentiment.T\n",
    "dfTestScores3ClustersSentiment.columns = ['scores']\n",
    "\n",
    "print('Scores for sentiment analysis with 3 clusters and no stopwords: ')\n",
    "dfTestScores3ClustersSentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Sentiment Analysis without stop words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 Stop words removal\n",
    "\n",
    "Finally, it is important to remove the stop-words.\n",
    "For this purpose, the 'stopwords' package of the 'nltk' library is used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "stopWords[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_stop_words(tokenized_reviews, stop_words):\n",
    "    tokenized_reviews_without_stopwords = []\n",
    "    for tokenized_review in tokenized_reviews:\n",
    "        tokenized_reviews_without_stopwords.append(\n",
    "            [word for word in tokenized_review if not word in stop_words]\n",
    "        )\n",
    "    return tokenized_reviews_without_stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lemmatizedTokenizedEnglishReviewsWithoutStopWords = remove_stop_words(\n",
    "    lemmatizedTokenizedEnglishReviews, stopWords)\n",
    "lemmatizedTokenizedEnglishReviewsWithoutStopWords[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2 Bigrams generation\n",
    "\n",
    "from notebook reviews_without_stopwords_analysis\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}